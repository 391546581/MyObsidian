#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIé©±åŠ¨çš„é›…æ€è¯æ±‡ç¬”è®°è‡ªåŠ¨ç”Ÿæˆå™¨
æ”¯æŒ: LLMè‡ªåŠ¨ç”Ÿæˆå†…å®¹ + AIå›¾ç‰‡è®°å¿† + æ‰¹é‡å¤„ç†
"""

import os
import json
import re
from datetime import datetime
from pathlib import Path
import requests
from typing import Dict, List, Optional

class AIVocabularyGenerator:
    """AIè¯æ±‡ç¬”è®°ç”Ÿæˆå™¨"""
    
    def __init__(self, api_key: str = None, api_base: str = None):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            api_key: LLM APIå¯†é’¥ (æ”¯æŒOpenAI/DeepSeek/Claudeç­‰)
            api_base: APIåŸºç¡€URL (å¯é€‰,ç”¨äºè‡ªå®šä¹‰ç«¯ç‚¹)
        """
        self.api_key = api_key or os.getenv('LLM_API_KEY')
        self.api_base = api_base or os.getenv('LLM_API_BASE', 'https://api.openai.com/v1')
        
        if not self.api_key:
            print("âš ï¸  æœªè®¾ç½®APIå¯†é’¥,å°†ä½¿ç”¨æ¨¡æ¿æ¨¡å¼(æ— AIç”Ÿæˆ)")
            self.ai_enabled = False
        else:
            self.ai_enabled = True
            print(f"âœ… AIæ¨¡å¼å·²å¯ç”¨: {self.api_base}")
    
    def generate_vocabulary_note(
        self, 
        word: str, 
        brand: str = 'Brand7',
        theme: str = None,
        include_image: bool = True
    ) -> Dict:
        """
        ç”Ÿæˆå®Œæ•´çš„è¯æ±‡ç¬”è®°å†…å®¹
        
        Args:
            word: å•è¯
            brand: ç­‰çº§ (Brand5/7/9)
            theme: ä¸»é¢˜ (æ•™è‚²/ç¯å¢ƒ/ç§‘æŠ€ç­‰)
            include_image: æ˜¯å¦ç”Ÿæˆè®°å¿†å›¾ç‰‡
            
        Returns:
            åŒ…å«æ‰€æœ‰ç¬”è®°å†…å®¹çš„å­—å…¸
        """
        print(f"\nğŸ” æ­£åœ¨ç”Ÿæˆè¯æ±‡ç¬”è®°: {word} ({brand})")
        
        if self.ai_enabled:
            return self._generate_with_ai(word, brand, theme, include_image)
        else:
            return self._generate_with_template(word, brand, theme)
    
    def _generate_with_ai(
        self, 
        word: str, 
        brand: str, 
        theme: Optional[str],
        include_image: bool
    ) -> Dict:
        """ä½¿ç”¨AIç”Ÿæˆè¯æ±‡ç¬”è®°"""
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(word, brand, theme)
        
        # è°ƒç”¨LLM API
        try:
            response = self._call_llm_api(prompt)
            note_data = self._parse_llm_response(response, word, brand)
            
            # ç”Ÿæˆè®°å¿†å›¾ç‰‡
            if include_image:
                image_path = self._generate_memory_image(word, note_data.get('memory_tip', ''))
                note_data['image_path'] = image_path
            
            print(f"âœ… AIç”Ÿæˆå®Œæˆ: {word}")
            return note_data
            
        except Exception as e:
            print(f"âŒ AIç”Ÿæˆå¤±è´¥: {e}")
            print("âš ï¸  å›é€€åˆ°æ¨¡æ¿æ¨¡å¼")
            return self._generate_with_template(word, brand, theme)
    
    def _build_prompt(self, word: str, brand: str, theme: Optional[str]) -> str:
        """æ„å»ºLLMæç¤ºè¯"""
        
        theme_context = f"ä¸»é¢˜: {theme}\n" if theme else ""
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é›…æ€è¯æ±‡æ•™å­¦ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹å•è¯ç”Ÿæˆå®Œæ•´çš„å­¦ä¹ ç¬”è®°ã€‚

å•è¯: {word}
ç­‰çº§: {brand}
{theme_context}
è¦æ±‚:
1. æä¾›ç²¾å‡†çš„ä¸­æ–‡é‡Šä¹‰å’Œè‹±æ–‡é‡Šä¹‰
2. ç»™å‡ºéŸ³æ ‡(IPAæ ¼å¼)
3. åˆ—å‡º3-5ä¸ªåŒä¹‰è¯,å¹¶æŒ‰Brand5/7/9åˆ†çº§
4. æä¾›3ä¸ªå›ºå®šæ­é…,æ¯ä¸ªæ­é…é…ä¸€ä¸ªé›…æ€çœŸé¢˜çº§åˆ«çš„ä¾‹å¥
5. åˆ›å»º5ä¸ªä¸åŒç±»å‹çš„å¤ä¹ å¡ç‰‡:
   - å¡ç‰‡1: åŸºç¡€é‡Šä¹‰
   - å¡ç‰‡2: å›ºå®šæ­é…å¡«ç©º
   - å¡ç‰‡3: åŒä¹‰è¯è¾¨æ
   - å¡ç‰‡4: å†™ä½œåº”ç”¨(å°†åŸºç¡€å¥å­æå‡åˆ°{brand}æ°´å¹³)
   - å¡ç‰‡5: å£è¯­åº”ç”¨(Part3é—®é¢˜å›ç­”)
6. æä¾›è¯æ ¹è¯ç¼€è®°å¿†æŠ€å·§
7. æ ‡æ³¨é€‚ç”¨åœºæ™¯å’Œä¸é€‚ç”¨åœºæ™¯
8. ç»™å‡º3ä¸ªçœŸå®ä¾‹å¥(åˆ†åˆ«æ¥è‡ª: é›…æ€å†™ä½œèŒƒæ–‡ã€å£è¯­é«˜åˆ†å›ç­”ã€å­¦æœ¯æ–‡ç« )

è¯·ä»¥JSONæ ¼å¼è¿”å›,åŒ…å«ä»¥ä¸‹å­—æ®µ:
{{
    "word": "å•è¯",
    "phonetic": "éŸ³æ ‡",
    "pos": "è¯æ€§",
    "cn_meaning": "ä¸­æ–‡é‡Šä¹‰",
    "en_definition": "è‹±æ–‡é‡Šä¹‰",
    "synonyms": {{
        "brand5": ["åŒä¹‰è¯1", "åŒä¹‰è¯2"],
        "brand7": ["åŒä¹‰è¯1", "åŒä¹‰è¯2"],
        "brand9": ["åŒä¹‰è¯1", "åŒä¹‰è¯2"]
    }},
    "collocations": [
        {{"phrase": "æ­é…", "example": "ä¾‹å¥", "translation": "ä¸­æ–‡ç¿»è¯‘"}},
        ...
    ],
    "flashcards": [
        {{
            "title": "å¡ç‰‡æ ‡é¢˜",
            "question": "é—®é¢˜",
            "answer": "ç­”æ¡ˆ",
            "notes": "è¡¥å……è¯´æ˜"
        }},
        ...
    ],
    "etymology": "è¯æ ¹è¯ç¼€åˆ†æ",
    "memory_tip": "è®°å¿†æŠ€å·§",
    "usage_scenarios": {{
        "suitable": ["é€‚ç”¨åœºæ™¯1", "é€‚ç”¨åœºæ™¯2"],
        "unsuitable": ["ä¸é€‚ç”¨åœºæ™¯1", "ä¸é€‚ç”¨åœºæ™¯2"]
    }},
    "examples": [
        {{
            "sentence": "ä¾‹å¥",
            "source": "æ¥æº",
            "translation": "ä¸­æ–‡ç¿»è¯‘"
        }},
        ...
    ],
    "related_words": ["å…³è”è¯1", "å…³è”è¯2", "å…³è”è¯3"]
}}

è¯·ç¡®ä¿å†…å®¹ä¸“ä¸šã€å‡†ç¡®ã€é€‚åˆé›…æ€è€ƒè¯•ã€‚"""
        
        return prompt
    
    def _call_llm_api(self, prompt: str, model: str = "gpt-4") -> str:
        """è°ƒç”¨LLM API"""
        
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': model,
            'messages': [
                {
                    'role': 'system',
                    'content': 'ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é›…æ€è¯æ±‡æ•™å­¦ä¸“å®¶,æ“…é•¿åˆ›å»ºé«˜è´¨é‡çš„è¯æ±‡å­¦ä¹ ç¬”è®°ã€‚'
                },
                {
                    'role': 'user',
                    'content': prompt
                }
            ],
            'temperature': 0.7,
            'max_tokens': 2000
        }
        
        response = requests.post(
            f'{self.api_base}/chat/completions',
            headers=headers,
            json=data,
            timeout=60
        )
        
        response.raise_for_status()
        result = response.json()
        
        return result['choices'][0]['message']['content']
    
    def _parse_llm_response(self, response: str, word: str, brand: str) -> Dict:
        """è§£æLLMè¿”å›çš„JSON"""
        
        # æå–JSONéƒ¨åˆ†
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            data = json.loads(json_str)
            data['brand'] = brand
            return data
        else:
            raise ValueError("æ— æ³•è§£æLLMè¿”å›çš„JSON")
    
    def _generate_memory_image(self, word: str, memory_tip: str) -> str:
        """ç”Ÿæˆè®°å¿†å›¾ç‰‡"""
        
        print(f"  ğŸ¨ æ­£åœ¨ç”Ÿæˆè®°å¿†å›¾ç‰‡: {word}")
        
        # è¿™é‡Œå¯ä»¥é›†æˆDALL-Eã€Stable Diffusionç­‰å›¾ç‰‡ç”ŸæˆAPI
        # ç¤ºä¾‹: ä½¿ç”¨OpenAI DALL-E
        
        try:
            prompt = f"""Create a memorable visual mnemonic for the English word "{word}". 
The image should help students remember the word through visual association.
Memory tip: {memory_tip}

Style: Clean, educational, colorful illustration suitable for vocabulary learning."""
            
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': 'dall-e-3',
                'prompt': prompt,
                'n': 1,
                'size': '1024x1024',
                'quality': 'standard'
            }
            
            response = requests.post(
                f'{self.api_base}/images/generations',
                headers=headers,
                json=data,
                timeout=120
            )
            
            response.raise_for_status()
            result = response.json()
            
            image_url = result['data'][0]['url']
            
            # ä¸‹è½½å›¾ç‰‡
            image_path = self._download_image(image_url, word)
            print(f"  âœ… å›¾ç‰‡å·²ç”Ÿæˆ: {image_path}")
            
            return image_path
            
        except Exception as e:
            print(f"  âš ï¸  å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _download_image(self, url: str, word: str) -> str:
        """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°"""
        
        # åˆ›å»ºå›¾ç‰‡ç›®å½•
        image_dir = Path('images/vocabulary')
        image_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½å›¾ç‰‡
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # ä¿å­˜å›¾ç‰‡
        image_path = image_dir / f"{word}_memory.png"
        with open(image_path, 'wb') as f:
            f.write(response.content)
        
        return str(image_path)
    
    def _generate_with_template(self, word: str, brand: str, theme: Optional[str]) -> Dict:
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ(æ— AI)"""
        
        return {
            'word': word,
            'brand': brand,
            'phonetic': 'å¾…è¡¥å……',
            'pos': 'å¾…è¡¥å……',
            'cn_meaning': 'å¾…è¡¥å……',
            'en_definition': 'å¾…è¡¥å……',
            'synonyms': {
                'brand5': [],
                'brand7': [],
                'brand9': []
            },
            'collocations': [],
            'flashcards': [],
            'etymology': 'å¾…è¡¥å……',
            'memory_tip': 'å¾…è¡¥å……',
            'usage_scenarios': {
                'suitable': [],
                'unsuitable': []
            },
            'examples': [],
            'related_words': []
        }
    
    def create_obsidian_note(self, note_data: Dict, output_dir: str = None) -> str:
        """åˆ›å»ºObsidianç¬”è®°æ–‡ä»¶"""
        
        word = note_data['word']
        brand = note_data['brand']
        
        # é»˜è®¤è¾“å‡ºç›®å½•
        if not output_dir:
            output_dir = f'06_Vocabulary/è¯æ±‡å­¦ä¹ ç³»ç»Ÿ/{brand}'
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆç¬”è®°å†…å®¹
        note_content = self._format_obsidian_note(note_data)
        
        # ä¿å­˜æ–‡ä»¶
        note_file = output_path / f"{word}.md"
        with open(note_file, 'w', encoding='utf-8') as f:
            f.write(note_content)
        
        print(f"âœ… ç¬”è®°å·²åˆ›å»º: {note_file}")
        return str(note_file)
    
    def _format_obsidian_note(self, data: Dict) -> str:
        """æ ¼å¼åŒ–ä¸ºObsidianç¬”è®°"""
        
        word = data['word']
        brand = data['brand']
        
        # YAML frontmatter
        frontmatter = f"""---
sr-due: {self._get_future_date(4)}
sr-interval: 4
sr-ease: 270
tags: [IELTS/{brand}, è¯æ±‡/{data.get('pos', 'å¾…è¡¥å……')}]
---

"""
        
        # ä¸»æ ‡é¢˜
        content = f"# {word}\n\n"
        
        # å¡ç‰‡1: åŸºç¡€é‡Šä¹‰
        content += f"""## å¡ç‰‡1: åŸºç¡€é‡Šä¹‰ #card

**å•è¯**: {word}  
**ç­‰çº§**: {brand}

**é—®**: {word} çš„ä¸­æ–‡æ„æ€æ˜¯?

**ç­”**: {data.get('cn_meaning', 'å¾…è¡¥å……')}

**è‹±æ–‡é‡Šä¹‰**: {data.get('en_definition', 'å¾…è¡¥å……')}

---

"""
        
        # å¡ç‰‡2: å›ºå®šæ­é…
        if data.get('collocations'):
            content += "## å¡ç‰‡2: å›ºå®šæ­é… #card\n\n**é—®**: ç”¨{word}å®Œæˆæ­é…:\n\n"
            for i, coll in enumerate(data['collocations'][:3], 1):
                phrase = coll['phrase'].replace(word, '_____')
                content += f"{i}. {phrase}\n"
            
            content += "\n**ç­”**:\n"
            for i, coll in enumerate(data['collocations'][:3], 1):
                content += f"{i}. {coll['phrase']}\n"
                content += f"   > {coll['example']}\n\n"
            
            content += "---\n\n"
        
        # å¡ç‰‡3: åŒä¹‰è¯è¾¨æ
        if data.get('synonyms'):
            syns = data['synonyms']
            content += f"""## å¡ç‰‡3: åŒä¹‰è¯åˆ†çº§ #card

**é—®**: {word} çš„åŒä¹‰è¯æœ‰å“ªäº›?æŒ‰Brand5/7/9åˆ†çº§

**ç­”**:
- **Brand5**: {', '.join(syns.get('brand5', []))}
- **Brand7**: {', '.join(syns.get('brand7', []))}
- **Brand9**: {', '.join(syns.get('brand9', []))}

---

"""
        
        # å¡ç‰‡4: å†™ä½œåº”ç”¨
        if data.get('flashcards'):
            writing_card = next((c for c in data['flashcards'] if 'å†™ä½œ' in c.get('title', '')), None)
            if writing_card:
                content += f"""## å¡ç‰‡4: å†™ä½œåº”ç”¨ #card

**é—®**: {writing_card.get('question', 'å¾…è¡¥å……')}

**ç­”**: {writing_card.get('answer', 'å¾…è¡¥å……')}

**è¯´æ˜**: {writing_card.get('notes', '')}

---

"""
        
        # å¡ç‰‡5: å£è¯­åº”ç”¨
        if data.get('flashcards'):
            speaking_card = next((c for c in data['flashcards'] if 'å£è¯­' in c.get('title', '')), None)
            if speaking_card:
                content += f"""## å¡ç‰‡5: å£è¯­åº”ç”¨ #card

**åœºæ™¯**: {speaking_card.get('question', 'å¾…è¡¥å……')}

**ç­”**: {speaking_card.get('answer', 'å¾…è¡¥å……')}

**äº®ç‚¹**: {speaking_card.get('notes', '')}

---

"""
        
        # è¡¥å……ç¬”è®°
        content += "## ğŸ“ è¡¥å……ç¬”è®°\n\n"
        
        # è¯æ ¹è®°å¿†
        if data.get('etymology'):
            content += f"### è¯æ ¹è¯ç¼€\n{data['etymology']}\n\n"
        
        # è®°å¿†æŠ€å·§
        if data.get('memory_tip'):
            content += f"### è®°å¿†æŠ€å·§\n{data['memory_tip']}\n\n"
        
        # è®°å¿†å›¾ç‰‡
        if data.get('image_path'):
            content += f"### è§†è§‰è®°å¿†\n![[{data['image_path']}]]\n\n"
        
        # å…³è”è¯æ±‡
        if data.get('related_words'):
            links = ' | '.join([f"[[{w}]]" for w in data['related_words']])
            content += f"### å…³è”è¯æ±‡\n{links}\n\n"
        
        # çœŸé¢˜ä¾‹å¥
        if data.get('examples'):
            content += "### çœŸé¢˜ä¾‹å¥\n\n"
            for ex in data['examples']:
                content += f"> {ex['sentence']}\n"
                content += f"> ğŸ“„ æ¥æº: {ex['source']}\n\n"
        
        # ä½¿ç”¨æç¤º
        if data.get('usage_scenarios'):
            scenarios = data['usage_scenarios']
            content += "## ğŸ¯ ä½¿ç”¨æç¤º\n\n"
            if scenarios.get('suitable'):
                content += "### âœ… æ¨èåœºæ™¯\n"
                for s in scenarios['suitable']:
                    content += f"- {s}\n"
                content += "\n"
            
            if scenarios.get('unsuitable'):
                content += "### âŒ é¿å…åœºæ™¯\n"
                for s in scenarios['unsuitable']:
                    content += f"- {s}\n"
                content += "\n"
        
        # æ ‡ç­¾
        content += f"\n---\n\n#IELTS/{brand} #è¯æ±‡/{data.get('pos', 'å¾…è¡¥å……')}\n"
        
        return frontmatter + content
    
    def _get_future_date(self, days: int) -> str:
        """è·å–æœªæ¥æ—¥æœŸ"""
        from datetime import timedelta
        future = datetime.now() + timedelta(days=days)
        return future.strftime('%Y-%m-%d')
    
    def batch_generate(
        self, 
        word_list: List[str], 
        brand: str = 'Brand7',
        theme: str = None,
        include_images: bool = False
    ) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆè¯æ±‡ç¬”è®°"""
        
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {len(word_list)} ä¸ªè¯æ±‡ç¬”è®°")
        print(f"ç­‰çº§: {brand}")
        if theme:
            print(f"ä¸»é¢˜: {theme}")
        print(f"ç”Ÿæˆå›¾ç‰‡: {'æ˜¯' if include_images else 'å¦'}\n")
        
        created_files = []
        
        for i, word in enumerate(word_list, 1):
            print(f"\n[{i}/{len(word_list)}] å¤„ç†: {word}")
            
            try:
                # ç”Ÿæˆç¬”è®°æ•°æ®
                note_data = self.generate_vocabulary_note(
                    word, 
                    brand=brand, 
                    theme=theme,
                    include_image=include_images
                )
                
                # åˆ›å»ºObsidianç¬”è®°
                file_path = self.create_obsidian_note(note_data)
                created_files.append(file_path)
                
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {word} - {e}")
                continue
        
        print(f"\nâœ… æ‰¹é‡ç”Ÿæˆå®Œæˆ! æˆåŠŸåˆ›å»º {len(created_files)} ä¸ªç¬”è®°")
        return created_files


def main():
    """ä¸»å‡½æ•° - äº¤äº’å¼ä½¿ç”¨"""
    
    print("="*60)
    print("ğŸ¯ AIé©±åŠ¨çš„é›…æ€è¯æ±‡ç¬”è®°ç”Ÿæˆå™¨")
    print("="*60)
    
    # æ£€æŸ¥APIå¯†é’¥
    api_key = input("\nè¯·è¾“å…¥LLM APIå¯†é’¥ (æˆ–æŒ‰Enterè·³è¿‡,ä½¿ç”¨æ¨¡æ¿æ¨¡å¼): ").strip()
    
    if api_key:
        api_base = input("API Base URL (é»˜è®¤OpenAI,æŒ‰Enterè·³è¿‡): ").strip()
        generator = AIVocabularyGenerator(
            api_key=api_key,
            api_base=api_base if api_base else None
        )
    else:
        generator = AIVocabularyGenerator()
    
    # é€‰æ‹©æ¨¡å¼
    print("\né€‰æ‹©æ¨¡å¼:")
    print("1. å•ä¸ªè¯æ±‡ç”Ÿæˆ")
    print("2. æ‰¹é‡ç”Ÿæˆ")
    
    mode = input("\nè¯·é€‰æ‹© (1/2): ").strip()
    
    if mode == '1':
        # å•ä¸ªè¯æ±‡
        word = input("\nè¯·è¾“å…¥å•è¯: ").strip()
        brand = input("ç­‰çº§ (Brand5/7/9, é»˜è®¤Brand7): ").strip() or 'Brand7'
        theme = input("ä¸»é¢˜ (å¯é€‰): ").strip() or None
        include_image = input("ç”Ÿæˆè®°å¿†å›¾ç‰‡? (y/n, é»˜è®¤n): ").strip().lower() == 'y'
        
        note_data = generator.generate_vocabulary_note(
            word, 
            brand=brand, 
            theme=theme,
            include_image=include_image
        )
        
        generator.create_obsidian_note(note_data)
        
    elif mode == '2':
        # æ‰¹é‡ç”Ÿæˆ
        print("\nè¯·è¾“å…¥å•è¯åˆ—è¡¨ (æ¯è¡Œä¸€ä¸ª,è¾“å…¥ç©ºè¡Œç»“æŸ):")
        word_list = []
        while True:
            word = input().strip()
            if not word:
                break
            word_list.append(word)
        
        if not word_list:
            print("æœªè¾“å…¥ä»»ä½•å•è¯")
            return
        
        brand = input("\nç­‰çº§ (Brand5/7/9, é»˜è®¤Brand7): ").strip() or 'Brand7'
        theme = input("ä¸»é¢˜ (å¯é€‰): ").strip() or None
        include_images = input("ç”Ÿæˆè®°å¿†å›¾ç‰‡? (y/n, é»˜è®¤n): ").strip().lower() == 'y'
        
        generator.batch_generate(
            word_list,
            brand=brand,
            theme=theme,
            include_images=include_images
        )
    
    print("\nâœ… å®Œæˆ!")


if __name__ == '__main__':
    main()
